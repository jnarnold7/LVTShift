from typing import Dict, List, Optional, Tuple, Union
import pandas as pd
import geopandas as gpd
from census import Census
from us import states
import requests
from shapely.geometry import Point, Polygon
import json
import os
from dotenv import load_dotenv
import numpy as np

# Load environment variables from .env file
load_dotenv()

def get_census_data(fips_code: str, year: int = 2022, api_key: str = None) -> pd.DataFrame:
    """
    Get Census demographic data for block groups in a given FIPS code.
    
    Args:
        fips_code (str): 5-digit FIPS code (state + county)
        year (int): Census year to query (default: 2022)
        api_key (str): Census API key. If None, will try to load from CENSUS_API_KEY environment variable.
    
    Returns:
        pd.DataFrame: DataFrame containing Census demographic data
        
    Raises:
        TypeError: If fips_code is not a string or year is not an int
        ValueError: If fips_code is not 5 digits or api_key is not provided
    """
    if not isinstance(fips_code, str):
        raise TypeError("fips_code must be a string")
    if not isinstance(year, int):
        raise TypeError("year must be an integer")
    if len(fips_code) != 5:
        raise ValueError("fips_code must be 5 digits (state + county)")
    
    # Try to get API key from environment variable if not provided
    if not api_key:
        api_key = os.getenv('CENSUS_API_KEY')
    if not api_key:
        raise ValueError("Census API key must be provided either as parameter or in CENSUS_API_KEY environment variable")

    # Initialize Census API
    c = Census(api_key)
    
    # Split FIPS code into state and county
    state_fips = fips_code[:2]
    county_fips = fips_code[2:]
    
    # Get block group data
    data = c.acs5.state_county_blockgroup(
        fields=['NAME',
                'B19013_001E',  # Median income
                'B01003_001E',  # Total population
                'B03002_003E',  # White alone
                'B03002_004E',  # Black alone
                'B03002_012E'],  # Hispanic/Latino
        state_fips=state_fips,
        county_fips=county_fips,
        blockgroup='*',  # All block groups
        year=year
    )

    # Convert to DataFrame
    df = pd.DataFrame(data)

    # Rename columns
    df = df.rename(columns={
        'B19013_001E': 'median_income',
        'B01003_001E': 'total_pop',
        'B03002_003E': 'white_pop',
        'B03002_004E': 'black_pop',
        'B03002_012E': 'hispanic_pop',
        'block group': 'census_block_group'  # Rename block group to census_block_group
    })

    # Create GEOID for block groups (state+county+tract+block group)
    df['state_fips'] = df['state']
    df['county_fips'] = df['county']
    df['tract_fips'] = df['tract']
    df['bg_fips'] = df['census_block_group']  # Use renamed column

    # Create standardized GEOID
    df['std_geoid'] = df['state_fips'] + df['county_fips'] + df['tract_fips'] + df['bg_fips']

    # Calculate minority and black percentages
    df['minority_pct'] = ((df['total_pop'] - df['white_pop']) / df['total_pop'] * 100).round(2)
    df['black_pct'] = (df['black_pop'] / df['total_pop'] * 100).round(2)

    return df

def get_census_blockgroups_shapefile_chunked(fips_code: str, max_retries: int = 3) -> gpd.GeoDataFrame:
    """
    Get Census Block Group shapefiles for large counties by fetching tract by tract.
    This approach handles counties like Cook County that are too large for a single request.
    
    Args:
        fips_code (str): 5-digit FIPS code (state + county)
        max_retries (int): Maximum number of retries for failed requests
    
    Returns:
        gpd.GeoDataFrame: GeoDataFrame containing Census Block Group boundaries
    """
    import time
    from typing import List
    
    if not isinstance(fips_code, str):
        raise TypeError("fips_code must be a string")
    if len(fips_code) != 5:
        raise ValueError("fips_code must be 5 digits (state + county)")

    state_fips = fips_code[:2]
    county_fips = fips_code[2:]
    
    # First, get all census tracts in the county
    tracts_url = "https://tigerweb.geo.census.gov/arcgis/rest/services/TIGERweb/Tracts_Blocks/MapServer/8/query"
    tracts_params = {
        'where': f"STATE='{state_fips}' AND COUNTY='{county_fips}'",
        'outFields': 'TRACT',
        'returnGeometry': 'false',
        'f': 'json'
    }
    
    print(f"üîç Getting census tracts for county {fips_code}...")
    
    try:
        tracts_response = requests.get(tracts_url, params=tracts_params)
        tracts_response.raise_for_status()
        tracts_data = tracts_response.json()
        
        if 'features' not in tracts_data:
            raise ValueError(f"No census tracts found for county {fips_code}")
            
        # Extract tract numbers
        tract_numbers = [feature['attributes']['TRACT'] for feature in tracts_data['features']]
        print(f"üìã Found {len(tract_numbers)} census tracts")
        
    except requests.RequestException as e:
        raise requests.RequestException(f"Failed to fetch census tracts: {str(e)}")

    # Now fetch block groups for each tract
    all_block_groups = []
    bg_url = "https://tigerweb.geo.census.gov/arcgis/rest/services/TIGERweb/Tracts_Blocks/MapServer/2/query"
    
    for i, tract in enumerate(tract_numbers):
        retry_count = 0
        while retry_count < max_retries:
            try:
                bg_params = {
                    'where': f"STATE='{state_fips}' AND COUNTY='{county_fips}' AND TRACT='{tract}'",
                    'outFields': '*',
                    'returnGeometry': 'true',
                    'f': 'geojson',
                    'outSR': '4326'  # WGS84
                }
                
                response = requests.get(bg_url, params=bg_params)
                response.raise_for_status()
                
                geojson_data = response.json()
                
                if 'features' in geojson_data and geojson_data['features']:
                    # Convert to GeoDataFrame
                    tract_gdf = gpd.GeoDataFrame.from_features(geojson_data['features'])
                    all_block_groups.append(tract_gdf)
                    
                print(f"‚úÖ Tract {tract} ({i+1}/{len(tract_numbers)}): {len(geojson_data.get('features', []))} block groups")
                
                # Small delay to be respectful to the API
                time.sleep(0.1)
                break  # Success, exit retry loop
                
            except requests.RequestException as e:
                retry_count += 1
                if retry_count >= max_retries:
                    print(f"‚ùå Failed to fetch tract {tract} after {max_retries} retries: {e}")
                    continue  # Skip this tract
                else:
                    print(f"‚ö†Ô∏è Retry {retry_count}/{max_retries} for tract {tract}: {e}")
                    time.sleep(1)  # Wait before retry
    
    if not all_block_groups:
        raise ValueError(f"No block group data successfully fetched for county {fips_code}")
    
    # Combine all block groups
    print(f"üîó Combining block groups from {len(all_block_groups)} tracts...")
    combined_gdf = gpd.pd.concat(all_block_groups, ignore_index=True)
    
    # Create standardized GEOID components
    combined_gdf['state_fips'] = combined_gdf['STATE']
    combined_gdf['county_fips'] = combined_gdf['COUNTY']  
    combined_gdf['tract_fips'] = combined_gdf['TRACT']
    combined_gdf['bg_fips'] = combined_gdf['BLKGRP']

    # Create standardized GEOID
    combined_gdf['std_geoid'] = combined_gdf['state_fips'] + combined_gdf['county_fips'] + combined_gdf['tract_fips'] + combined_gdf['bg_fips']
    
    print(f"üéâ Successfully fetched {len(combined_gdf)} total block groups for county {fips_code}")
    
    return combined_gdf

def get_census_blockgroups_shapefile(fips_code: str) -> gpd.GeoDataFrame:
    """
    Get Census Block Group shapefiles for a given FIPS code from the Census TIGERweb service.
    Automatically handles large counties (like Cook County) by using a chunked approach.
    
    Args:
        fips_code (str): 5-digit FIPS code (state + county)
    
    Returns:
        gpd.GeoDataFrame: GeoDataFrame containing Census Block Group boundaries
        
    Raises:
        TypeError: If fips_code is not a string
        ValueError: If fips_code is not 5 digits
        requests.RequestException: If API request fails
    """
    if not isinstance(fips_code, str):
        raise TypeError("fips_code must be a string")
    if len(fips_code) != 5:
        raise ValueError("fips_code must be 5 digits (state + county)")

    # Large counties that need chunked approach
    # Cook County, IL (17031) and other large metropolitan counties
    large_counties = [
        '17031',  # Cook County, IL (Chicago)
        '06037',  # Los Angeles County, CA
        '48201',  # Harris County, TX (Houston)  
        '04013',  # Maricopa County, AZ (Phoenix)
        '06073',  # San Diego County, CA
        '06059',  # Orange County, CA
        '36081',  # Queens County, NY
        '36047',  # Kings County, NY (Brooklyn)
        '12086',  # Miami-Dade County, FL
        '53033',  # King County, WA (Seattle)
    ]
    
    if fips_code in large_counties:
        print(f"üîß Using chunked approach for large county {fips_code}")
        return get_census_blockgroups_shapefile_chunked(fips_code)

    # TIGERweb REST API endpoint
    base_url = "https://tigerweb.geo.census.gov/arcgis/rest/services/TIGERweb/Tracts_Blocks/MapServer/2/query"
    
    # Query parameters
    params = {
        'where': f"STATE='{fips_code[:2]}' AND COUNTY='{fips_code[2:]}'",
        'outFields': '*',
        'returnGeometry': 'true',
        'f': 'geojson',
        'outSR': '4326'  # WGS84
    }
    
    try:
        response = requests.get(base_url, params=params)
        response.raise_for_status()
        
        # Check if we got a rejection message (usually HTML error page)
        content_type = response.headers.get('content-type', '').lower()
        if 'html' in content_type:
            print(f"‚ö†Ô∏è Received HTML response instead of JSON - likely request rejected")
            print(f"üîß Falling back to chunked approach for county {fips_code}")
            return get_census_blockgroups_shapefile_chunked(fips_code)
        
        geojson_data = response.json()
        
        # Convert to GeoDataFrame
        gdf = gpd.GeoDataFrame.from_features(geojson_data['features'])
        
        # Create standardized GEOID components
        gdf['state_fips'] = gdf['STATE']
        gdf['county_fips'] = gdf['COUNTY']
        gdf['tract_fips'] = gdf['TRACT']
        gdf['bg_fips'] = gdf['BLKGRP']

        # Create standardized GEOID
        gdf['std_geoid'] = gdf['state_fips'] + gdf['county_fips'] + gdf['tract_fips'] + gdf['bg_fips']
        
        return gdf
        
    except (requests.RequestException, ValueError, KeyError) as e:
        print(f"‚ö†Ô∏è Direct request failed: {str(e)}")
        print(f"üîß Falling back to chunked approach for county {fips_code}")
        return get_census_blockgroups_shapefile_chunked(fips_code)

def match_to_census_blockgroups(
    gdf: gpd.GeoDataFrame,
    census_gdf: gpd.GeoDataFrame,
    join_type: str = "left"
) -> gpd.GeoDataFrame:
    """
    Match each row in a GeoDataFrame to its corresponding Census Block Group using spatial join.
    
    Args:
        gdf (gpd.GeoDataFrame): Input GeoDataFrame to match
        census_gdf (gpd.GeoDataFrame): Census Block Group boundaries GeoDataFrame
        join_type (str): Type of join to perform ('left', 'right', 'inner', 'outer')
    
    Returns:
        gpd.GeoDataFrame: Input GeoDataFrame with Census Block Group data appended
        
    Raises:
        TypeError: If inputs are not GeoDataFrames
        ValueError: If join_type is invalid
    """
    if not isinstance(gdf, gpd.GeoDataFrame):
        raise TypeError("gdf must be a GeoDataFrame")
    if not isinstance(census_gdf, gpd.GeoDataFrame):
        raise TypeError("census_gdf must be a GeoDataFrame")
    if join_type not in ['left', 'right', 'inner', 'outer']:
        raise ValueError("join_type must be one of: 'left', 'right', 'inner', 'outer'")

    # Ensure both GDFs have same CRS
    if gdf.crs != census_gdf.crs:
        census_gdf = census_gdf.to_crs(gdf.crs)

    # Preserve original polygon geometry
    original_geometry = gdf.geometry.copy()

    # Compute centroids in a projected CRS to avoid geographic centroid warnings
    if gdf.crs is not None and getattr(gdf.crs, "is_geographic", False):
        gdf_for_centroid = gdf.to_crs("EPSG:3857")
        gdf_centroids = gdf_for_centroid.copy()
        gdf_centroids.geometry = gdf_for_centroid.geometry.centroid
        # Project centroids back to the original CRS for the join
        gdf_centroids = gdf_centroids.to_crs(gdf.crs)
    else:
        gdf_centroids = gdf.copy()
        gdf_centroids.geometry = gdf_centroids.geometry.centroid

    # Perform spatial join using centroids to avoid overlapping boundary issues
    joined = gpd.sjoin(gdf_centroids, census_gdf, how=join_type, predicate='within')

    # Drop unnecessary columns from the join
    if 'index_right' in joined.columns:
        joined = joined.drop(columns=['index_right'])

    # Restore original polygon geometries and CRS
    joined.geometry = original_geometry.loc[joined.index].values
    if joined.crs is None and gdf.crs is not None:
        joined.set_crs(gdf.crs, inplace=True)

    return joined

def get_census_data_with_boundaries(
    fips_code: str,
    year: int = 2022,
    api_key: str = None
) -> Tuple[pd.DataFrame, gpd.GeoDataFrame]:
    """
    Get both Census demographic data and boundary files for block groups in a FIPS code.
    
    Args:
        fips_code (str): 5-digit FIPS code (state + county)
        year (int): Census year to query (default: 2022)
        api_key (str): Census API key. If None, will try to load from CENSUS_API_KEY environment variable.
    
    Returns:
        Tuple[pd.DataFrame, gpd.GeoDataFrame]: 
            - Census demographic data DataFrame
            - Census Block Group boundaries GeoDataFrame
            
    Raises:
        TypeError: If inputs have wrong types
        ValueError: If inputs have invalid values
        requests.RequestException: If API requests fail
    """
    # Get demographic data
    census_data = get_census_data(fips_code, year, api_key)
    
    # Get boundary files
    census_boundaries = get_census_blockgroups_shapefile(fips_code)
    
    # Merge demographic data with boundaries
    # Use suffixes to avoid duplicate column names
    census_boundaries = census_boundaries.merge(
        census_data,
        on='std_geoid',
        how='left',
        suffixes=('', '_census')
    )
    
    # Drop duplicate columns that might have been created with the _census suffix
    census_boundaries = census_boundaries.loc[:, ~census_boundaries.columns.str.endswith('_census')]
    
    return census_data, census_boundaries

def enrich_shapefile_with_census(
    shapefile: Union[str, gpd.GeoDataFrame],
    fips_code: str,
    year: int = 2022,
    api_key: str = None
) -> Tuple[gpd.GeoDataFrame, gpd.GeoDataFrame]:
    """
    Takes a shapefile (or GeoDataFrame) and enriches it with Census Block Group data.
    Also returns the Census Block Group data separately.
    
    Args:
        shapefile (Union[str, gpd.GeoDataFrame]): Either a path to a shapefile or a GeoDataFrame
        fips_code (str): 5-digit FIPS code (state + county)
        year (int): Census year to query (default: 2022)
        api_key (str): Census API key. If None, will try to load from CENSUS_API_KEY environment variable.
    
    Returns:
        Tuple[gpd.GeoDataFrame, gpd.GeoDataFrame]:
            - Original shapefile enriched with Census Block Group data
            - Census Block Group boundaries with demographic data
            
    Raises:
        TypeError: If inputs have wrong types
        ValueError: If inputs have invalid values
        requests.RequestException: If API requests fail
        FileNotFoundError: If shapefile path is invalid
    """
    # Input validation
    if not isinstance(fips_code, str):
        raise TypeError("fips_code must be a string")
    if not isinstance(year, int):
        raise TypeError("year must be an integer")
    if len(fips_code) != 5:
        raise ValueError("fips_code must be 5 digits (state + county)")
    if not api_key:
        raise ValueError("Census API key must be provided")

    # Load the shapefile if string path provided
    if isinstance(shapefile, str):
        try:
            gdf = gpd.read_file(shapefile)
        except Exception as e:
            raise FileNotFoundError(f"Failed to read shapefile at {shapefile}: {str(e)}")
    elif isinstance(shapefile, gpd.GeoDataFrame):
        gdf = shapefile.copy()
    else:
        raise TypeError("shapefile must be either a file path (str) or a GeoDataFrame")
    
    # Check if the GeoDataFrame has a CRS and set one if it doesn't
    if gdf.crs is None:
        raise ValueError("Input GeoDataFrame must have a coordinate reference system (CRS) defined. "
                         "Use gdf.set_crs() to set a CRS before calling this function.")

    # Get census data and boundaries
    census_data, census_boundaries = get_census_data_with_boundaries(
        fips_code=fips_code,
        year=year,
        api_key=api_key
    )
    
    # Ensure census boundaries have a CRS
    if census_boundaries.crs is None:
        # Set a default CRS (EPSG:4326 - WGS84) if none exists
        census_boundaries = census_boundaries.set_crs("EPSG:4326")

    # Match shapefile to census block groups
    # Use a list of columns to keep from census_boundaries to avoid duplicates
    census_cols_to_keep = ['std_geoid', 'median_income', 'total_pop', 'white_pop', 
                          'black_pop', 'hispanic_pop']
    
    # First perform the spatial join
    enriched_gdf = match_to_census_blockgroups(
        gdf=gdf,
        census_gdf=census_boundaries[['geometry', 'std_geoid'] + 
                                    [col for col in census_cols_to_keep if col != 'std_geoid']],
        join_type="left"
    )

    return enriched_gdf, census_boundaries

def get_census_blockgroups_from_ftp(fips_code: str, year: int = 2022) -> gpd.GeoDataFrame:
    """
    Alternative method: Download Census Block Group shapefiles from the Census FTP server.
    Useful for very large counties where the TIGERweb API fails.
    
    Args:
        fips_code (str): 5-digit FIPS code (state + county)
        year (int): Census year (default: 2022)
    
    Returns:
        gpd.GeoDataFrame: GeoDataFrame containing Census Block Group boundaries
    """
    import zipfile
    import tempfile
    import urllib.request
    import os
    
    if not isinstance(fips_code, str):
        raise TypeError("fips_code must be a string")
    if len(fips_code) != 5:
        raise ValueError("fips_code must be 5 digits (state + county)")

    state_fips = fips_code[:2]
    
    # Census FTP URL for block groups by state
    ftp_url = f"https://www2.census.gov/geo/tiger/TIGER{year}/BG/tl_{year}_{state_fips}_bg.zip"
    
    print(f"üì• Downloading block groups for state {state_fips} from Census FTP...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        zip_path = os.path.join(temp_dir, f"tl_{year}_{state_fips}_bg.zip")
        
        try:
            # Download the zip file
            urllib.request.urlretrieve(ftp_url, zip_path)
            print(f"‚úÖ Downloaded {os.path.getsize(zip_path):,} bytes")
            
            # Extract the zip file
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(temp_dir)
            
            # Find the shapefile
            shp_files = [f for f in os.listdir(temp_dir) if f.endswith('.shp')]
            if not shp_files:
                raise FileNotFoundError("No shapefile found in downloaded zip")
            
            shp_path = os.path.join(temp_dir, shp_files[0])
            
            # Read the shapefile
            print(f"üìä Reading shapefile: {shp_files[0]}")
            gdf = gpd.read_file(shp_path)
            
            # Filter to the specific county
            county_fips = fips_code[2:]
            filtered_gdf = gdf[gdf['COUNTYFP'] == county_fips].copy()
            
            print(f"üéØ Filtered to {len(filtered_gdf)} block groups in county {fips_code}")
            
            if len(filtered_gdf) == 0:
                raise ValueError(f"No block groups found for county {fips_code}")
            
            # Rename columns to match TIGERweb format
            filtered_gdf = filtered_gdf.rename(columns={
                'STATEFP': 'STATE',
                'COUNTYFP': 'COUNTY', 
                'TRACTCE': 'TRACT',
                'BLKGRPCE': 'BLKGRP'
            })
            
            # Create standardized GEOID components
            filtered_gdf['state_fips'] = filtered_gdf['STATE']
            filtered_gdf['county_fips'] = filtered_gdf['COUNTY']
            filtered_gdf['tract_fips'] = filtered_gdf['TRACT']
            filtered_gdf['bg_fips'] = filtered_gdf['BLKGRP']

            # Create standardized GEOID
            filtered_gdf['std_geoid'] = filtered_gdf['state_fips'] + filtered_gdf['county_fips'] + filtered_gdf['tract_fips'] + filtered_gdf['bg_fips']
            
            # Ensure CRS is WGS84
            if filtered_gdf.crs != 'EPSG:4326':
                filtered_gdf = filtered_gdf.to_crs('EPSG:4326')
            
            return filtered_gdf
            
        except Exception as e:
            raise Exception(f"Failed to download/process Census data from FTP: {str(e)}")


def calculate_median_percentage_by_quintile(
    df: pd.DataFrame, 
    group_col: str, 
    value_col: str,
    weight_col: Optional[str] = None
) -> pd.DataFrame:
    """
    Calculate median percentage by quintile groups with optional weighting.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe
    group_col : str
        Column to create quintiles from (e.g., 'median_income')
    value_col : str
        Value column to calculate median for (e.g., 'tax_change_pct')
    weight_col : str, optional
        Column to use for weighting (e.g., 'parcel_count')
        
    Returns:
    --------
    pd.DataFrame
        Summary with quintiles and median values
    """
    from viz import weighted_median
    
    # Filter positive values for income-based quintiles
    work_df = df.copy()
    if group_col == 'median_income' and group_col in work_df.columns:
        work_df = work_df[work_df['median_income'] > 0].copy()
    
    # Create quintiles
    work_df[f'{group_col}_quintile'] = pd.qcut(
        work_df[group_col], 
        5, 
        labels=["Q1 (Lowest)", "Q2", "Q3", "Q4", "Q5 (Highest)"]
    )
    
    # Calculate weighted median by quintile
    def calc_weighted_median(group):
        if weight_col and weight_col in group.columns:
            weights = group[weight_col].values
        else:
            weights = np.ones(len(group))
        return weighted_median(group[value_col].values, weights)
    
    summary = work_df.groupby(f'{group_col}_quintile').apply(
        lambda g: pd.Series({
            'count': len(g),
            'median_value': calc_weighted_median(g),
            'mean_value': g[value_col].mean(),
            f'mean_{group_col}': g[group_col].mean()
        })
    ).reset_index()
    
    return summary


def match_parcels_to_demographics(
    parcels_gdf: gpd.GeoDataFrame,
    demographics_df: pd.DataFrame,
    block_groups_gdf: gpd.GeoDataFrame,
    demographic_id_col: str = 'std_geoid',
    block_group_id_col: str = 'GEOID'
) -> gpd.GeoDataFrame:
    """
    Match parcels to demographic data via spatial join with block groups.
    
    Parameters:
    -----------
    parcels_gdf : gpd.GeoDataFrame
        Parcel data with geometry
    demographics_df : pd.DataFrame
        Census demographic data
    block_groups_gdf : gpd.GeoDataFrame
        Block group boundaries
    demographic_id_col : str, default='std_geoid'
        ID column in demographics data
    block_group_id_col : str, default='GEOID'
        ID column in block groups data
        
    Returns:
    --------
    gpd.GeoDataFrame
        Parcels with demographic data attached
    """
    # Ensure consistent CRS
    if parcels_gdf.crs != block_groups_gdf.crs:
        block_groups_gdf = block_groups_gdf.to_crs(parcels_gdf.crs)
    
    # Spatial join parcels to block groups
    parcels_with_bg = gpd.sjoin(
        parcels_gdf, 
        block_groups_gdf[[block_group_id_col, 'geometry']], 
        how='left', 
        predicate='within'
    )
    
    # Merge with demographic data
    merged = parcels_with_bg.merge(
        demographics_df,
        left_on=block_group_id_col,
        right_on=demographic_id_col,
        how='left'
    )
    
    return merged


def create_demographic_summary(
    df: pd.DataFrame,
    group_col: str,
    tax_change_col: str = 'tax_change',
    tax_change_pct_col: str = 'tax_change_pct',
    current_tax_col: str = 'current_tax',
    new_tax_col: str = 'new_tax'
) -> pd.DataFrame:
    """
    Create demographic summary statistics for tax impact analysis.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe with demographic and tax data
    group_col : str
        Column to group by (e.g., 'std_geoid' for block groups)
    tax_change_col : str, default='tax_change'
        Tax change column name
    tax_change_pct_col : str, default='tax_change_pct'
        Tax change percentage column name
    current_tax_col : str, default='current_tax'
        Current tax column name
    new_tax_col : str, default='new_tax'
        New tax column name
        
    Returns:
    --------
    pd.DataFrame
        Demographic summary with tax impact statistics
    """
    # Calculate tax change percentage if not present
    work_df = df.copy()
    if tax_change_pct_col not in work_df.columns:
        work_df[tax_change_pct_col] = np.where(
            work_df[current_tax_col] != 0,
            (work_df[tax_change_col] / work_df[current_tax_col]) * 100,
            0
        )
    
    # Group by demographic unit
    summary = work_df.groupby(group_col).agg({
        tax_change_col: ['sum', 'count', 'mean'],
        tax_change_pct_col: 'mean',
        current_tax_col: 'sum',
        new_tax_col: 'sum',
        # Include demographic columns if they exist
        **{col: 'first' for col in ['median_income', 'minority_pct', 'black_pct', 'total_pop'] 
           if col in work_df.columns}
    })
    
    # Flatten column names
    summary.columns = ['_'.join(col).strip() if col[1] else col[0] for col in summary.columns.values]
    summary.columns = [col.replace('_first', '') for col in summary.columns]
    
    # Rename for clarity
    rename_dict = {
        f'{tax_change_col}_sum': 'total_tax_change',
        f'{tax_change_col}_count': 'parcel_count',
        f'{tax_change_col}_mean': 'mean_tax_change',
        f'{tax_change_pct_col}_mean': 'mean_tax_change_pct',
        f'{current_tax_col}_sum': 'total_current_tax',
        f'{new_tax_col}_sum': 'total_new_tax'
    }
    summary = summary.rename(columns=rename_dict)
    
    # Calculate total tax change percentage
    summary['total_tax_change_pct'] = (
        (summary['total_new_tax'] - summary['total_current_tax']) / 
        summary['total_current_tax']
    ) * 100
    summary['total_tax_change_pct'] = summary['total_tax_change_pct'].replace([np.inf, -np.inf], 0).fillna(0)
    
    # Reset index
    summary = summary.reset_index()
    
    return summary
